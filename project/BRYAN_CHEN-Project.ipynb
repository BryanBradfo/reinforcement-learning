{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b73e4931",
   "metadata": {},
   "source": [
    "# Final Project: Load balancing\n",
    "\n",
    "Time is discrete. Let $Q_1$ and $Q_2$ denote the total number of jobs in the server $1$ and $2$, respectively.\n",
    "Every time slot, the dispatcher observes ($Q_1$, $Q_2$) and takes the action $a_i$, $i = 1, 2$, that dispatches a potential new incoming job to server $i$. The cost in every time slot is $Q_1 + Q_2$, independently of the\n",
    "action.\n",
    "\n",
    "In every time slot, there is a probability $\\lambda$ of having a new job, which will be dispatched to server\n",
    "ai. If $Q_i > 0$, with probability $\\mu_i$ a job from server i will depart. For simplicity, we will assume that\n",
    "in every time slot, only one event can happen, i.e., either an arrival, or a departure from servers $1$ or\n",
    "$2$\n",
    "\n",
    "For example, in the state $(2,1)$, if the dispatcher takes the action $1$, we have that with probability $\\lambda$ the next state will be $(3,1)$, with probability $\\mu_1 (1,1)$, , with probability $\\mu_2 (2,0)$, and with probability 1 − $\\mu_1$ − $\\mu_2$ − $\\lambda$ the next state will be $(2,1)$.\n",
    "\n",
    "We will assume that there is an upper bound for both $Q_1$ and $Q_2$ equal to 20. If either $Q_1 = 20$ or $Q_2 = 20$, no new incoming job will arrive to the system.\n",
    "\n",
    "Let us choose $\\mu_1=0.2, \\mu_2=0.4$ and $\\lambda = 0.3$. Throughout we take that the discounting factor is $\\gamma = 0.99$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e57e00",
   "metadata": {},
   "source": [
    "## 1. MDP\n",
    "\n",
    "### 1.1. Policy Evaluation\n",
    "\n",
    "Assume the random policy that dispatches every job with probability $0.5$ to either queue $1$ and $2$.\n",
    "\n",
    "- Write down the Bellman equation that characterizes the value function for this policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Calculate the value function for this policy using Iterative Policy Evaluation. Due to the contraction principle, the initial vector can be arbitrary, so you can take V (Q1, Q2) = 0, for all (Q1, Q2). To stop iterating, you can take as a criterion that the difference between two iterations must be smaller than some small δ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "lamda = 0.3\n",
    "mu1 = 0.2\n",
    "mu2 = 0.4\n",
    "gamma = 0.99\n",
    "p1 = 0.5\n",
    "p2 = 0.5\n",
    "p_Q1_Q2_a1 = p1 * (1-mu1-mu2-lamda)\n",
    "p_Q1_Q2_a2 = p2 * (1-mu1-mu2-lamda)\n",
    "\n",
    "p_Q1p1_Q2_a1 = p1 * lamda\n",
    "p_Q1_Q2p1_a2 = p2 * lamda\n",
    "\n",
    "p_Q1m1_Q2_a1 = p1 * mu1\n",
    "p_Q1m1_Q2_a2 = p2 * mu1\n",
    "\n",
    "p_Q1_Q2m1_a1 = p1 * mu2\n",
    "p_Q1_Q2m1_a2 = p2 * mu2\n",
    "\n",
    "\n",
    "def value1(epsilon):\n",
    "    Vn = np.zeros((21,21))\n",
    "    Vn_p1 = np.zeros((21,21))\n",
    "    first_boucle = True\n",
    "    count = 0\n",
    "\n",
    "    while np.max(abs((Vn_p1 - Vn))) > epsilon or first_boucle:\n",
    "        count +=1\n",
    "        first_boucle = False\n",
    "        action = np.random.randint(0,2)\n",
    "        Vn = Vn_p1.copy()\n",
    "        for Q1 in range(21):\n",
    "            for Q2 in range(21):\n",
    "                rec = - (Q1 + Q2)\n",
    "\n",
    "                if Q1 == 20:\n",
    "                    Vn_Q1p1_Q2 = Vn[Q1,Q2]\n",
    "                else:\n",
    "                    Vn_Q1p1_Q2 = Vn[Q1+1,Q2]\n",
    "                \n",
    "                if Q2 == 20:\n",
    "                    Vn_Q1_Q2p1 = Vn[Q1,Q2]\n",
    "                else:\n",
    "                    Vn_Q1_Q2p1 = Vn[Q1, Q2+1]\n",
    "\n",
    "                if Q1 == 0:\n",
    "                    Vn_Q1m1_Q2 = Vn[Q1,Q2]\n",
    "                else:\n",
    "                    Vn_Q1m1_Q2 = Vn[Q1-1,Q2]\n",
    "                \n",
    "                if Q2 == 0:\n",
    "                    Vn_Q1_Q2m1 = Vn[Q1,Q2]\n",
    "                else:\n",
    "                    Vn_Q1_Q2m1 = Vn[Q1, Q2-1]\n",
    "\n",
    "\n",
    "                Vn_p1[Q1,Q2] = rec + gamma * (p_Q1_Q2_a1 * Vn[Q1,Q2]\n",
    "                                    + p_Q1p1_Q2_a1 * Vn_Q1p1_Q2\n",
    "                                    + p_Q1_Q2p1_a2 * Vn_Q1_Q2p1\n",
    "                                    + p_Q1m1_Q2_a1 * Vn_Q1m1_Q2\n",
    "                                    + p_Q1_Q2m1_a1 * Vn_Q1_Q2m1\n",
    "                                    + (p_Q1_Q2_a2 * Vn[Q1,Q2]\n",
    "                                    + p_Q1m1_Q2_a2 * Vn_Q1m1_Q2\n",
    "                                    + p_Q1_Q2m1_a2 * Vn_Q1_Q2m1))\n",
    "        \n",
    "    return Vn_p1, count\n",
    "                    \n",
    "epsilon = 1e-5\n",
    "Value_final, count_final = value1(epsilon)\n",
    "print(count_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add plot with titles\n",
    "plt.imshow(Value_final,cmap='hot')\n",
    "plt.colorbar()\n",
    "plt.title('Value function')\n",
    "plt.xlabel('Q1')\n",
    "plt.ylabel('Q2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimal Control\n",
    "\n",
    "In this part you are asked to find the optimal policy to dispatch incoming jobs.\n",
    "\n",
    "- Write down the Bellman equation that characterizes the optimal policy.\n",
    "\n",
    ">>> \n",
    "\n",
    "- Solve numerically the optimality value function by Value Iteration Algorithm.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lamda = 0.3\n",
    "mu1 = 0.2\n",
    "mu2 = 0.4\n",
    "gamma = 0.99\n",
    "\n",
    "p_Q1_Q2_a1 = (1-mu1-mu2-lamda)\n",
    "p_Q1_Q2_a2 = (1-mu1-mu2-lamda)\n",
    "\n",
    "p_Q1p1_Q2_a1 = lamda\n",
    "p_Q1_Q2p1_a2 = lamda\n",
    "\n",
    "p_Q1m1_Q2_a1 = mu1\n",
    "p_Q1m1_Q2_a2 = mu1\n",
    "\n",
    "p_Q1_Q2m1_a1 = mu2\n",
    "p_Q1_Q2m1_a2 = mu2\n",
    "\n",
    "actiontab = np.zeros((21,21))\n",
    "\n",
    "\n",
    "def optimal_value(epsilon):\n",
    "    Vn = np.zeros((21,21))\n",
    "    Vn_p1 = np.zeros((21,21))\n",
    "    first_boucle = True\n",
    "    count = 0\n",
    "\n",
    "    while np.max(abs(Vn_p1 - Vn)) > epsilon or first_boucle:\n",
    "        count +=1\n",
    "        first_boucle = False\n",
    "        action = np.random.randint(0,2)\n",
    "        Vn = Vn_p1.copy()\n",
    "        for Q1 in range(21):\n",
    "            for Q2 in range(21):\n",
    "                rec = - (Q1 + Q2)\n",
    "\n",
    "                if Q1 == 20:\n",
    "                    if Q2 == 20:\n",
    "                        Vn_Q1p1_Q2 = Vn[Q1,Q2]\n",
    "                        Vn_Q1_Q2p1 = 0\n",
    "                        actiontab[Q1,Q2] = 3\n",
    "                    else:\n",
    "                        Vn_Q1_Q2p1 = Vn[Q1, Q2+1]\n",
    "                        Vn_Q1p1_Q2 = 0\n",
    "                        actiontab[Q1,Q2] = 2\n",
    "                else:\n",
    "                    if Q2 == 20:\n",
    "                        Vn_Q1_Q2p1 = 0\n",
    "                        Vn_Q1p1_Q2 = Vn[Q1+1 ,Q2]\n",
    "                        actiontab[Q1,Q2] = 1\n",
    "                    else:\n",
    "                        if Vn[Q1, Q2+1] >= Vn[Q1+1,Q2]:\n",
    "                            Vn_Q1_Q2p1 = Vn[Q1, Q2+1]\n",
    "                            Vn_Q1p1_Q2 = 0\n",
    "                            actiontab[Q1,Q2] = 2\n",
    "                        else:\n",
    "                            Vn_Q1_Q2p1 = 0\n",
    "                            Vn_Q1p1_Q2 = Vn[Q1+1 ,Q2]\n",
    "                            actiontab[Q1,Q2] = 1\n",
    "\n",
    "                if Q1 == 0:\n",
    "                    Vn_Q1m1_Q2 = Vn[Q1,Q2]\n",
    "                else:\n",
    "                    Vn_Q1m1_Q2 = Vn[Q1-1,Q2]\n",
    "                \n",
    "                if Q2 == 0:\n",
    "                    Vn_Q1_Q2m1 = Vn[Q1,Q2]\n",
    "                else:\n",
    "                    Vn_Q1_Q2m1 = Vn[Q1, Q2-1]\n",
    "\n",
    "\n",
    "                Vn_p1[Q1,Q2] = rec + gamma * (p_Q1_Q2_a1 * Vn[Q1,Q2]\n",
    "                                    + p_Q1p1_Q2_a1 * Vn_Q1p1_Q2\n",
    "                                    + p_Q1_Q2p1_a2 * Vn_Q1_Q2p1\n",
    "                                    + p_Q1m1_Q2_a1 * Vn_Q1m1_Q2\n",
    "                                    + p_Q1_Q2m1_a1 * Vn_Q1_Q2m1)\n",
    "        \n",
    "    return Vn_p1, actiontab, count\n",
    "                    \n",
    "epsilon = 0.01\n",
    "Value_final_optimal, optimal_politic, count = optimal_value(epsilon)\n",
    "print(optimal_politic)\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(Value_final_optimal,cmap='hot')\n",
    "plt.colorbar()\n",
    "plt.title('Optimal value function')\n",
    "plt.xlabel('Q1')\n",
    "plt.ylabel('Q2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot for the optimal action and add values on the cmap\n",
    "\n",
    "plt.imshow(optimal_politic,cmap='gist_rainbow')\n",
    "\n",
    "# Add values to each cell\n",
    "for i in range(optimal_politic.shape[0]):\n",
    "    for j in range(optimal_politic.shape[1]):\n",
    "        value = int(optimal_politic[i, j])\n",
    "        plt.annotate(str(value), (j, i), fontsize=12, ha='center', va='center')\n",
    "\n",
    "\n",
    "plt.colorbar()\n",
    "plt.title('Optimal action')\n",
    "plt.xlabel('Q1')\n",
    "plt.ylabel('Q2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Represent on the plane the optimal action as a function of the state (Q1, Q2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vecteurs = np.zeros((21,21,2))\n",
    "for i in range(21):\n",
    "    for j in range(21):\n",
    "        new_vector = (optimal_politic[i][j] - 1) * np.array([0,1]) - (optimal_politic[i][j] - 2) * np.array([1,0])\n",
    "        vecteurs[i,j] = new_vector\n",
    "print(vecteurs)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(Value_final_optimal,cmap='hot')\n",
    "ax.quiver(vecteurs[:,:,1], vecteurs[:,:,0], angles='xy', scale_units = 'xy', scale=1)\n",
    "plt.title('Superposition of the optimal value function and the optimal action')\n",
    "plt.xlabel('Q1')\n",
    "plt.ylabel('Q2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Compare the performances obtained with the random policy and the optimal one, how can you conclude that the optimal policy performs better ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(Value_final_optimal))\n",
    "print(np.mean(Value_final))\n",
    "print(np.array((Value_final_optimal - Value_final) > 0))\n",
    "# On peut remarquer que la difference entre la politique optimal et la politique aléatoire est bien positive. Ainsi la première est surement meilleure que la deuxième."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that $ \\mathbb{E}[V_{\\text{optimal}}] > \\mathbb{E}[V_{\\text{random}}] $, therefore we can conclude that the optimal policy performs better than the random policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Carry out a one-step improvement on the random policy, what do you observe ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "lamda = 0.3\n",
    "mu1 = 0.2\n",
    "mu2 = 0.4\n",
    "gamma = 0.99\n",
    "p1 = 0.5\n",
    "p2 = 0.5\n",
    "p_Q1_Q2_a1 = p1 * (1-mu1-mu2-lamda)\n",
    "p_Q1_Q2_a2 = p2 * (1-mu1-mu2-lamda)\n",
    "\n",
    "p_Q1p1_Q2_a1 = p1 * lamda\n",
    "p_Q1_Q2p1_a2 = p2 * lamda\n",
    "\n",
    "p_Q1m1_Q2_a1 = p1 * mu1\n",
    "p_Q1m1_Q2_a2 = p2 * mu1\n",
    "\n",
    "p_Q1_Q2m1_a1 = p1 * mu2\n",
    "p_Q1_Q2m1_a2 = p2 * mu2\n",
    "\n",
    "\n",
    "def one_step_improvement(epsilon):\n",
    "    Vn = np.zeros((21,21))\n",
    "    Vn_p1 = np.zeros((21,21))\n",
    "    Vn_p1_a1 = np.zeros((21,21))\n",
    "    Vn_p1_a2 = np.zeros((21,21))\n",
    "    first_boucle = True\n",
    "    count = 0\n",
    "\n",
    "    while np.max(abs((Vn_p1 - Vn))) > epsilon or first_boucle:\n",
    "        count +=1\n",
    "        first_boucle = False\n",
    "        action = np.random.randint(0,2)\n",
    "        Vn = Vn_p1.copy()\n",
    "        for Q1 in range(21):\n",
    "            for Q2 in range(21):\n",
    "                rec = - (Q1 + Q2)\n",
    "\n",
    "                if Q1 == 20:\n",
    "                    Vn_Q1p1_Q2 = Vn[Q1,Q2]\n",
    "                else:\n",
    "                    Vn_Q1p1_Q2 = Vn[Q1+1,Q2]\n",
    "                \n",
    "                if Q2 == 20:\n",
    "                    Vn_Q1_Q2p1 = Vn[Q1,Q2]\n",
    "                else:\n",
    "                    Vn_Q1_Q2p1 = Vn[Q1, Q2+1]\n",
    "\n",
    "                if Q1 == 0:\n",
    "                    Vn_Q1m1_Q2 = Vn[Q1,Q2]\n",
    "                else:\n",
    "                    Vn_Q1m1_Q2 = Vn[Q1-1,Q2]\n",
    "                \n",
    "                if Q2 == 0:\n",
    "                    Vn_Q1_Q2m1 = Vn[Q1,Q2]\n",
    "                else:\n",
    "                    Vn_Q1_Q2m1 = Vn[Q1, Q2-1]\n",
    "\n",
    "\n",
    "                Vn_p1[Q1,Q2] = rec + gamma * (p_Q1_Q2_a1 * Vn[Q1,Q2]\n",
    "                                    + p_Q1p1_Q2_a1 * Vn_Q1p1_Q2\n",
    "                                    + p_Q1_Q2p1_a2 * Vn_Q1_Q2p1\n",
    "                                    + p_Q1m1_Q2_a1 * Vn_Q1m1_Q2\n",
    "                                    + p_Q1_Q2m1_a1 * Vn_Q1_Q2m1\n",
    "                                    + (p_Q1_Q2_a2 * Vn[Q1,Q2]\n",
    "                                    + p_Q1m1_Q2_a2 * Vn_Q1m1_Q2\n",
    "                                    + p_Q1_Q2m1_a2 * Vn_Q1_Q2m1))\n",
    "\n",
    "    Vn = Vn_p1.copy()\n",
    "    for Q1 in range(21):\n",
    "        for Q2 in range(21):\n",
    "            rec = - (Q1 + Q2)\n",
    "\n",
    "            if Q1 == 20:\n",
    "                Vn_Q1p1_Q2 = Vn[Q1,Q2]\n",
    "            else:\n",
    "                Vn_Q1p1_Q2 = Vn[Q1+1,Q2]\n",
    "            \n",
    "            if Q2 == 20:\n",
    "                Vn_Q1_Q2p1 = Vn[Q1,Q2]\n",
    "            else:\n",
    "                Vn_Q1_Q2p1 = Vn[Q1, Q2+1]\n",
    "\n",
    "            if Q1 == 0:\n",
    "                Vn_Q1m1_Q2 = Vn[Q1,Q2]\n",
    "            else:\n",
    "                Vn_Q1m1_Q2 = Vn[Q1-1,Q2]\n",
    "            \n",
    "            if Q2 == 0:\n",
    "                Vn_Q1_Q2m1 = Vn[Q1,Q2]\n",
    "            else:\n",
    "                Vn_Q1_Q2m1 = Vn[Q1, Q2-1]\n",
    "\n",
    "\n",
    "            Vn_p1_a1[Q1,Q2] = rec + gamma * (p_Q1_Q2_a1 * Vn[Q1,Q2]\n",
    "                                + p_Q1p1_Q2_a1 * Vn_Q1p1_Q2\n",
    "                                + p_Q1m1_Q2_a1 * Vn_Q1m1_Q2\n",
    "                                + p_Q1_Q2m1_a1 * Vn_Q1_Q2m1)\n",
    "            \n",
    "            Vn_p1_a2[Q1,Q2] = rec + gamma * (p_Q1_Q2p1_a2 * Vn_Q1_Q2p1\n",
    "                                + p_Q1_Q2_a2 * Vn[Q1,Q2]\n",
    "                                + p_Q1m1_Q2_a2 * Vn_Q1m1_Q2\n",
    "                                + p_Q1_Q2m1_a2 * Vn_Q1_Q2m1)\n",
    "            \n",
    "            if Vn_p1_a1[Q1,Q2] >= Vn_p1_a2[Q1,Q2]:\n",
    "                Vn_p1[Q1,Q2] = Vn_p1_a1[Q1,Q2]\n",
    "            else:\n",
    "                Vn_p1[Q1,Q2] = Vn_p1_a2[Q1,Q2]\n",
    "\n",
    "\n",
    "    return Vn_p1, count\n",
    "                    \n",
    "epsilon = 1e-5\n",
    "Value_final_one_step_imp, count_final_one_step_imp = one_step_improvement(epsilon)\n",
    "print(count_final_one_step_imp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,3, figsize=(15, 4))\n",
    "ax_0=ax[0].imshow(Value_final,cmap='hot')\n",
    "ax[0].set_ylabel('Q2')\n",
    "ax[0].set_xlabel('Q1')\n",
    "ax[0].set_title('Random policy', fontsize=14)\n",
    "sm_0 = plt.cm.ScalarMappable(cmap=ax_0.get_cmap(), norm=ax_0.norm)\n",
    "sm_0.set_array([])\n",
    "plt.colorbar(sm_0, ax=ax[0], orientation='vertical')\n",
    "\n",
    "ax_1=ax[1].imshow(Value_final_one_step_imp,cmap='hot')\n",
    "ax[1].set_ylabel('Q2')\n",
    "ax[1].set_xlabel('Q1')\n",
    "ax[1].set_title('With one step improvement', fontsize=14)\n",
    "sm_1 = plt.cm.ScalarMappable(cmap=ax_1.get_cmap(), norm=ax_1.norm)\n",
    "sm_1.set_array([])\n",
    "plt.colorbar(sm_1, ax=ax[1], orientation='vertical')\n",
    "\n",
    "\n",
    "ax_2=ax[2].imshow(Value_final_one_step_imp - Value_final,cmap='hot')\n",
    "ax[2].set_ylabel('Q2')\n",
    "ax[2].set_xlabel('Q1')\n",
    "ax[2].set_title('Difference of them', fontsize=14)\n",
    "sm_2 = plt.cm.ScalarMappable(cmap=ax_2.get_cmap(), norm=ax_2.norm)\n",
    "sm_2.set_array([])\n",
    "plt.colorbar(sm_2, ax=ax[2], orientation='vertical')\n",
    "\n",
    "# Adjust the spacing between subplots\n",
    "plt.subplots_adjust(wspace=0.35)\n",
    "plt.show()\n",
    "print((Value_final_one_step_imp - Value_final) > 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'algorithme avec un \"one-step improvement\" produit des résultats supérieurs à celui basé sur l'évaluation d'une politique aléatoire pour plusieurs raisons :\n",
    "\n",
    "1. **Local Adjustment:** La méthode \"one-step improvement\" effectue une ajustement local de la politique en fonction de la valeur actuelle des états, tandis que l'évaluation d'une politique aléatoire peut ne pas exploiter pleinement les informations locales disponibles.\n",
    "\n",
    "2. **Immediate Benefit:** La procédure \"one-step improvement\" prend en compte les retours attendus immédiats pour chaque action dans chaque état, ce qui peut rapidement conduire à des améliorations locales significatives.\n",
    "\n",
    "3. **Use of Current Information:** En utilisant la fonction de valeur actuelle, \"one-step improvement\" prend des décisions plus informées, tenant compte des connaissances actuelles de l'environnement, ce qui peut conduire à des choix d'actions plus optimaux.\n",
    "\n",
    "En somme, cette approche profite des informations actuelles pour effectuer des ajustements ciblés, ce qui peut rapidement améliorer les performances par rapport à une politique aléatoire. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tabular Model-Free control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Policy Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume the random policy that dispatches every job with probability 0.5 to either queue 1 and 2. For the learning parameter you can use $\\alpha_n = \\frac{1}{n}$\n",
    "\n",
    "- Implement TD(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement TD(0) with probability 0.5 to either queue 1 and 2, and take alpha = 1/n\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "lamda = 0.3\n",
    "\n",
    "mu1 = 0.2\n",
    "mu2 = 0.4\n",
    "\n",
    "gamma = 0.99\n",
    "\n",
    "p1 = 0.5\n",
    "\n",
    "p2 = 0.5\n",
    "# Simulate the environment\n",
    "\n",
    "def simulate_env(Q1, Q2, action):\n",
    "    Q1_minus_possible = True\n",
    "    Q2_minus_possible = True\n",
    "    Q1_plus_possible = True\n",
    "    Q2_plus_possible = True\n",
    "    one_action_done = False\n",
    "\n",
    "    if Q1 == 20:\n",
    "        Q1_plus_possible = False\n",
    "        if Q2 == 20:\n",
    "            Q2_plus_possible = False\n",
    "            action = 2\n",
    "        else:\n",
    "            Q2_plus_possible = True\n",
    "            action = 1\n",
    "    else:\n",
    "        Q1_plus_possible = True\n",
    "        if Q2 == 20:\n",
    "            Q2_plus_possible = False\n",
    "            action = 0\n",
    "        else:\n",
    "            Q2_plus_possible = True\n",
    "\n",
    "    if Q1 == 0:\n",
    "        Q1_minus_possible = False\n",
    "    else:\n",
    "        Q1_minus_possible = True\n",
    "    \n",
    "    if Q2 == 0:\n",
    "        Q2_minus_possible = False\n",
    "    else:\n",
    "        Q2_minus_possible = True\n",
    "\n",
    "    if action == 0:\n",
    "        if np.random.uniform() < lamda and Q1_plus_possible:\n",
    "            Q1 += 1\n",
    "            one_action_done = True\n",
    "    else:\n",
    "        if np.random.uniform() < lamda and Q2_plus_possible:\n",
    "            Q2 += 1\n",
    "            one_action_done = True\n",
    "    \n",
    "    if np.random.uniform() < mu1 and Q1_minus_possible and not one_action_done:\n",
    "        Q1 -= 1\n",
    "        one_action_done = True\n",
    "    if np.random.uniform() < mu2 and Q2_minus_possible and not one_action_done:\n",
    "        Q2 -= 1\n",
    "    \n",
    "    return Q1, Q2\n",
    "\n",
    "# Implement TD(0)\n",
    "\n",
    "def TD0(alpha, n):\n",
    "    Vn = np.zeros((21,21))\n",
    "    Tirage = np.zeros((21,21))\n",
    "    first_boucle = True\n",
    "    count = 0\n",
    "    Q1 = 0\n",
    "    Q2 = 0\n",
    "\n",
    "    while count < n:\n",
    "        count +=1\n",
    "        for Q1 in range(21):\n",
    "            for Q2 in range(21):\n",
    "                Tirage[Q1,Q2] +=1\n",
    "                alpha = 1/(Tirage[Q1,Q2])\n",
    "                # alpha = 1/math.sqrt(math.sqrt(Tirage[Q1,Q2]))\n",
    "                action = np.random.randint(0,2)\n",
    "                Q1_new, Q2_new = simulate_env(Q1, Q2, action)\n",
    "                Vn[Q1,Q2] = Vn[Q1,Q2] + alpha * (- (Q1 + Q2) + gamma * Vn[Q1_new,Q2_new] - Vn[Q1,Q2])\n",
    "\n",
    "                # Q1 = np.random.randint(0,21)\n",
    "                # Q2 = np.random.randint(0,21)\n",
    "    \n",
    "    return Vn\n",
    "\n",
    "n = 3000 \n",
    "\n",
    "Value_final_TD0 = TD0(1, n)\n",
    "print(Value_final_TD0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(Value_final_TD0,cmap='hot')\n",
    "plt.colorbar()\n",
    "plt.title('Value function for TD(0)')\n",
    "plt.xlabel('Q1')\n",
    "plt.ylabel('Q2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Compare the obtained value function with the results of Section 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,3, figsize=(15, 4))\n",
    "ax_0=ax[0].imshow(Value_final,cmap='hot')\n",
    "ax[0].set_ylabel('Q2')\n",
    "ax[0].set_xlabel('Q1')\n",
    "ax[0].set_title('Random policy', fontsize=14)\n",
    "sm_0 = plt.cm.ScalarMappable(cmap=ax_0.get_cmap(), norm=ax_0.norm)\n",
    "sm_0.set_array([])\n",
    "plt.colorbar(sm_0, ax=ax[0], orientation='vertical')\n",
    "\n",
    "ax_1=ax[1].imshow(Value_final_TD0,cmap='hot')\n",
    "ax[1].set_ylabel('Q2')\n",
    "ax[1].set_xlabel('Q1')\n",
    "ax[1].set_title('TD(0)', fontsize=14)\n",
    "sm_1 = plt.cm.ScalarMappable(cmap=ax_1.get_cmap(), norm=ax_1.norm)\n",
    "sm_1.set_array([])\n",
    "plt.colorbar(sm_1, ax=ax[1], orientation='vertical')\n",
    "\n",
    "\n",
    "ax_2=ax[2].imshow(Value_final_TD0 - Value_final,cmap='hot')\n",
    "ax[2].set_ylabel('Q2')\n",
    "ax[2].set_xlabel('Q1')\n",
    "ax[2].set_title('Difference of them', fontsize=14)\n",
    "sm_2 = plt.cm.ScalarMappable(cmap=ax_2.get_cmap(), norm=ax_2.norm)\n",
    "sm_2.set_array([])\n",
    "plt.colorbar(sm_2, ax=ax[2], orientation='vertical')\n",
    "\n",
    "# Adjust the spacing between subplots\n",
    "plt.subplots_adjust(wspace=0.35)\n",
    "plt.show()\n",
    "print((Value_final_TD0 - Value_final) > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,3, figsize=(15, 4))\n",
    "ax_0=ax[0].imshow(Value_final_one_step_imp,cmap='hot')\n",
    "ax[0].set_ylabel('Q2')\n",
    "ax[0].set_xlabel('Q1')\n",
    "ax[0].set_title('With one step improvement', fontsize=14)\n",
    "sm_0 = plt.cm.ScalarMappable(cmap=ax_0.get_cmap(), norm=ax_0.norm)\n",
    "sm_0.set_array([])\n",
    "plt.colorbar(sm_0, ax=ax[0], orientation='vertical')\n",
    "\n",
    "ax_1=ax[1].imshow(Value_final_TD0,cmap='hot')\n",
    "ax[1].set_ylabel('Q2')\n",
    "ax[1].set_xlabel('Q1')\n",
    "ax[1].set_title('TD(0)', fontsize=14)\n",
    "sm_1 = plt.cm.ScalarMappable(cmap=ax_1.get_cmap(), norm=ax_1.norm)\n",
    "sm_1.set_array([])\n",
    "plt.colorbar(sm_1, ax=ax[1], orientation='vertical')\n",
    "\n",
    "\n",
    "ax_2=ax[2].imshow(Value_final_TD0 - Value_final_one_step_imp,cmap='hot')\n",
    "ax[2].set_ylabel('Q2')\n",
    "ax[2].set_xlabel('Q1')\n",
    "ax[2].set_title('Difference of them', fontsize=14)\n",
    "sm_2 = plt.cm.ScalarMappable(cmap=ax_2.get_cmap(), norm=ax_2.norm)\n",
    "sm_2.set_array([])\n",
    "plt.colorbar(sm_2, ax=ax[2], orientation='vertical')\n",
    "\n",
    "# Adjust the spacing between subplots\n",
    "plt.subplots_adjust(wspace=0.35)\n",
    "plt.show()\n",
    "print((Value_final_TD0 - Value_final_one_step_imp) > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,3, figsize=(15, 4))\n",
    "ax_0=ax[0].imshow(Value_final_optimal,cmap='hot')\n",
    "ax[0].set_ylabel('Q2')\n",
    "ax[0].set_xlabel('Q1')\n",
    "ax[0].set_title('Optimal value function', fontsize=14)\n",
    "sm_0 = plt.cm.ScalarMappable(cmap=ax_0.get_cmap(), norm=ax_0.norm)\n",
    "sm_0.set_array([])\n",
    "plt.colorbar(sm_0, ax=ax[0], orientation='vertical')\n",
    "\n",
    "ax_1=ax[1].imshow(Value_final_TD0,cmap='hot')\n",
    "ax[1].set_ylabel('Q2')\n",
    "ax[1].set_xlabel('Q1')\n",
    "ax[1].set_title('TD(0)', fontsize=14)\n",
    "sm_1 = plt.cm.ScalarMappable(cmap=ax_1.get_cmap(), norm=ax_1.norm)\n",
    "sm_1.set_array([])\n",
    "plt.colorbar(sm_1, ax=ax[1], orientation='vertical')\n",
    "\n",
    "\n",
    "ax_2=ax[2].imshow(Value_final_TD0 - Value_final_optimal,cmap='hot')\n",
    "ax[2].set_ylabel('Q2')\n",
    "ax[2].set_xlabel('Q1')\n",
    "ax[2].set_title('Difference of them', fontsize=14)\n",
    "sm_2 = plt.cm.ScalarMappable(cmap=ax_2.get_cmap(), norm=ax_2.norm)\n",
    "sm_2.set_array([])\n",
    "plt.colorbar(sm_2, ax=ax[2], orientation='vertical')\n",
    "\n",
    "# Adjust the spacing between subplots\n",
    "plt.subplots_adjust(wspace=0.35)\n",
    "plt.show()\n",
    "\n",
    "print((Value_final_TD0 - Value_final_optimal) > 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Explore other alternatives for $\\alpha_n$, for example constant, to see if convergence improves\n",
    "- TODO: Copy code from above and change the alpha by: a constant, and 1/sqrt(count), comment the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement TD(0) with probability 0.5 to either queue 1 and 2, and take alpha = 1/n\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "lamda = 0.3\n",
    "\n",
    "mu1 = 0.2\n",
    "mu2 = 0.4\n",
    "\n",
    "gamma = 0.99\n",
    "\n",
    "p1 = 0.5\n",
    "\n",
    "p2 = 0.5\n",
    "# Simulate the environment\n",
    "\n",
    "def simulate_env(Q1, Q2, action):\n",
    "    Q1_minus_possible = True\n",
    "    Q2_minus_possible = True\n",
    "    Q1_plus_possible = True\n",
    "    Q2_plus_possible = True\n",
    "    one_action_done = False\n",
    "\n",
    "    if Q1 == 20:\n",
    "        Q1_plus_possible = False\n",
    "        if Q2 == 20:\n",
    "            Q2_plus_possible = False\n",
    "            action = 2\n",
    "        else:\n",
    "            Q2_plus_possible = True\n",
    "            action = 1\n",
    "    else:\n",
    "        Q1_plus_possible = True\n",
    "        if Q2 == 20:\n",
    "            Q2_plus_possible = False\n",
    "            action = 0\n",
    "        else:\n",
    "            Q2_plus_possible = True\n",
    "\n",
    "    if Q1 == 0:\n",
    "        Q1_minus_possible = False\n",
    "    else:\n",
    "        Q1_minus_possible = True\n",
    "    \n",
    "    if Q2 == 0:\n",
    "        Q2_minus_possible = False\n",
    "    else:\n",
    "        Q2_minus_possible = True\n",
    "\n",
    "    if action == 0:\n",
    "        if np.random.uniform() < lamda and Q1_plus_possible:\n",
    "            Q1 += 1\n",
    "            one_action_done = True\n",
    "    else:\n",
    "        if np.random.uniform() < lamda and Q2_plus_possible:\n",
    "            Q2 += 1\n",
    "            one_action_done = True\n",
    "    \n",
    "    if np.random.uniform() < mu1 and Q1_minus_possible and not one_action_done:\n",
    "        Q1 -= 1\n",
    "        one_action_done = True\n",
    "    if np.random.uniform() < mu2 and Q2_minus_possible and not one_action_done:\n",
    "        Q2 -= 1\n",
    "    \n",
    "    return Q1, Q2\n",
    "\n",
    "# Implement TD(0)\n",
    "\n",
    "def TD0(alpha, n):\n",
    "    Vn = np.zeros((21,21))\n",
    "    Tirage = np.zeros((21,21))\n",
    "    first_boucle = True\n",
    "    count = 0\n",
    "    Q1 = 0\n",
    "    Q2 = 0\n",
    "\n",
    "    while count < n:\n",
    "        count +=1\n",
    "        for Q1 in range(21):\n",
    "            for Q2 in range(21):\n",
    "                Tirage[Q1,Q2] +=1\n",
    "                alpha = 1\n",
    "                # alpha = 1/math.sqrt(math.sqrt(Tirage[Q1,Q2]))\n",
    "                action = np.random.randint(0,2)\n",
    "                Q1_new, Q2_new = simulate_env(Q1, Q2, action)\n",
    "                Vn[Q1,Q2] = Vn[Q1,Q2] + alpha * (- (Q1 + Q2) + gamma * Vn[Q1_new,Q2_new] - Vn[Q1,Q2])\n",
    "\n",
    "                # Q1 = np.random.randint(0,21)\n",
    "                # Q2 = np.random.randint(0,21)\n",
    "    \n",
    "    return Vn\n",
    "\n",
    "n = 3000\n",
    "\n",
    "Value_final_TD0 = TD0(1, n)\n",
    "print(Value_final_TD0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(Value_final_TD0,cmap='hot')\n",
    "plt.colorbar()\n",
    "plt.title('Value function for TD(0) with alpha = 1')\n",
    "plt.xlabel('Q1')\n",
    "plt.ylabel('Q2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,3, figsize=(15, 4))\n",
    "ax_0=ax[0].imshow(Value_final,cmap='hot')\n",
    "ax[0].set_ylabel('Q2')\n",
    "ax[0].set_xlabel('Q1')\n",
    "ax[0].set_title('Random policy', fontsize=14)\n",
    "sm_0 = plt.cm.ScalarMappable(cmap=ax_0.get_cmap(), norm=ax_0.norm)\n",
    "sm_0.set_array([])\n",
    "plt.colorbar(sm_0, ax=ax[0], orientation='vertical')\n",
    "\n",
    "ax_1=ax[1].imshow(Value_final_TD0,cmap='hot')\n",
    "ax[1].set_ylabel('Q2')\n",
    "ax[1].set_xlabel('Q1')\n",
    "ax[1].set_title('TD(0)', fontsize=14)\n",
    "sm_1 = plt.cm.ScalarMappable(cmap=ax_1.get_cmap(), norm=ax_1.norm)\n",
    "sm_1.set_array([])\n",
    "plt.colorbar(sm_1, ax=ax[1], orientation='vertical')\n",
    "\n",
    "\n",
    "ax_2=ax[2].imshow(Value_final_TD0 - Value_final,cmap='hot')\n",
    "ax[2].set_ylabel('Q2')\n",
    "ax[2].set_xlabel('Q1')\n",
    "ax[2].set_title('Difference of them', fontsize=14)\n",
    "sm_2 = plt.cm.ScalarMappable(cmap=ax_2.get_cmap(), norm=ax_2.norm)\n",
    "sm_2.set_array([])\n",
    "plt.colorbar(sm_2, ax=ax[2], orientation='vertical')\n",
    "\n",
    "# Adjust the spacing between subplots\n",
    "plt.subplots_adjust(wspace=0.35)\n",
    "plt.show()\n",
    "print((Value_final_TD0 - Value_final) > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,3, figsize=(15, 4))\n",
    "ax_0=ax[0].imshow(Value_final_one_step_imp,cmap='hot')\n",
    "ax[0].set_ylabel('Q2')\n",
    "ax[0].set_xlabel('Q1')\n",
    "ax[0].set_title('With one step improvement', fontsize=14)\n",
    "sm_0 = plt.cm.ScalarMappable(cmap=ax_0.get_cmap(), norm=ax_0.norm)\n",
    "sm_0.set_array([])\n",
    "plt.colorbar(sm_0, ax=ax[0], orientation='vertical')\n",
    "\n",
    "ax_1=ax[1].imshow(Value_final_TD0,cmap='hot')\n",
    "ax[1].set_ylabel('Q2')\n",
    "ax[1].set_xlabel('Q1')\n",
    "ax[1].set_title('TD(0)', fontsize=14)\n",
    "sm_1 = plt.cm.ScalarMappable(cmap=ax_1.get_cmap(), norm=ax_1.norm)\n",
    "sm_1.set_array([])\n",
    "plt.colorbar(sm_1, ax=ax[1], orientation='vertical')\n",
    "\n",
    "\n",
    "ax_2=ax[2].imshow(Value_final_TD0 - Value_final_one_step_imp,cmap='hot')\n",
    "ax[2].set_ylabel('Q2')\n",
    "ax[2].set_xlabel('Q1')\n",
    "ax[2].set_title('Difference of them', fontsize=14)\n",
    "sm_2 = plt.cm.ScalarMappable(cmap=ax_2.get_cmap(), norm=ax_2.norm)\n",
    "sm_2.set_array([])\n",
    "plt.colorbar(sm_2, ax=ax[2], orientation='vertical')\n",
    "\n",
    "# Adjust the spacing between subplots\n",
    "plt.subplots_adjust(wspace=0.35)\n",
    "plt.show()\n",
    "print((Value_final_TD0 - Value_final_one_step_imp) > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,3, figsize=(15, 4))\n",
    "ax_0=ax[0].imshow(Value_final_optimal,cmap='hot')\n",
    "ax[0].set_ylabel('Q2')\n",
    "ax[0].set_xlabel('Q1')\n",
    "ax[0].set_title('Optimal value function', fontsize=14)\n",
    "sm_0 = plt.cm.ScalarMappable(cmap=ax_0.get_cmap(), norm=ax_0.norm)\n",
    "sm_0.set_array([])\n",
    "plt.colorbar(sm_0, ax=ax[0], orientation='vertical')\n",
    "\n",
    "ax_1=ax[1].imshow(Value_final_TD0,cmap='hot')\n",
    "ax[1].set_ylabel('Q2')\n",
    "ax[1].set_xlabel('Q1')\n",
    "ax[1].set_title('TD(0)', fontsize=14)\n",
    "sm_1 = plt.cm.ScalarMappable(cmap=ax_1.get_cmap(), norm=ax_1.norm)\n",
    "sm_1.set_array([])\n",
    "plt.colorbar(sm_1, ax=ax[1], orientation='vertical')\n",
    "\n",
    "\n",
    "ax_2=ax[2].imshow(Value_final_TD0 - Value_final_optimal,cmap='hot')\n",
    "ax[2].set_ylabel('Q2')\n",
    "ax[2].set_xlabel('Q1')\n",
    "ax[2].set_title('Difference of them', fontsize=14)\n",
    "sm_2 = plt.cm.ScalarMappable(cmap=ax_2.get_cmap(), norm=ax_2.norm)\n",
    "sm_2.set_array([])\n",
    "plt.colorbar(sm_2, ax=ax[2], orientation='vertical')\n",
    "\n",
    "# Adjust the spacing between subplots\n",
    "plt.subplots_adjust(wspace=0.35)\n",
    "plt.show()\n",
    "\n",
    "print((Value_final_TD0 - Value_final_optimal) > 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement TD(0) with probability 0.5 to either queue 1 and 2, and take alpha = 1/n\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "lamda = 0.3\n",
    "\n",
    "mu1 = 0.2\n",
    "mu2 = 0.4\n",
    "\n",
    "gamma = 0.99\n",
    "\n",
    "p1 = 0.5\n",
    "\n",
    "p2 = 0.5\n",
    "# Simulate the environment\n",
    "\n",
    "def simulate_env(Q1, Q2, action):\n",
    "    Q1_minus_possible = True\n",
    "    Q2_minus_possible = True\n",
    "    Q1_plus_possible = True\n",
    "    Q2_plus_possible = True\n",
    "    one_action_done = False\n",
    "\n",
    "    if Q1 == 20:\n",
    "        Q1_plus_possible = False\n",
    "        if Q2 == 20:\n",
    "            Q2_plus_possible = False\n",
    "            action = 2\n",
    "        else:\n",
    "            Q2_plus_possible = True\n",
    "            action = 1\n",
    "    else:\n",
    "        Q1_plus_possible = True\n",
    "        if Q2 == 20:\n",
    "            Q2_plus_possible = False\n",
    "            action = 0\n",
    "        else:\n",
    "            Q2_plus_possible = True\n",
    "\n",
    "    if Q1 == 0:\n",
    "        Q1_minus_possible = False\n",
    "    else:\n",
    "        Q1_minus_possible = True\n",
    "    \n",
    "    if Q2 == 0:\n",
    "        Q2_minus_possible = False\n",
    "    else:\n",
    "        Q2_minus_possible = True\n",
    "\n",
    "    if action == 0:\n",
    "        if np.random.uniform() < lamda and Q1_plus_possible:\n",
    "            Q1 += 1\n",
    "            one_action_done = True\n",
    "    else:\n",
    "        if np.random.uniform() < lamda and Q2_plus_possible:\n",
    "            Q2 += 1\n",
    "            one_action_done = True\n",
    "    \n",
    "    if np.random.uniform() < mu1 and Q1_minus_possible and not one_action_done:\n",
    "        Q1 -= 1\n",
    "        one_action_done = True\n",
    "    if np.random.uniform() < mu2 and Q2_minus_possible and not one_action_done:\n",
    "        Q2 -= 1\n",
    "    \n",
    "    return Q1, Q2\n",
    "\n",
    "# Implement TD(0)\n",
    "\n",
    "def TD0(alpha, n):\n",
    "    Vn = np.zeros((21,21))\n",
    "    Tirage = np.zeros((21,21))\n",
    "    first_boucle = True\n",
    "    count = 0\n",
    "    Q1 = 0\n",
    "    Q2 = 0\n",
    "\n",
    "    while count < n:\n",
    "        count +=1\n",
    "        for Q1 in range(21):\n",
    "            for Q2 in range(21):\n",
    "                Tirage[Q1,Q2] +=1\n",
    "                # alpha = 1\n",
    "                alpha = 1/math.sqrt(math.sqrt(Tirage[Q1,Q2]))\n",
    "                action = np.random.randint(0,2)\n",
    "                Q1_new, Q2_new = simulate_env(Q1, Q2, action)\n",
    "                Vn[Q1,Q2] = Vn[Q1,Q2] + alpha * (- (Q1 + Q2) + gamma * Vn[Q1_new,Q2_new] - Vn[Q1,Q2])\n",
    "\n",
    "                # Q1 = np.random.randint(0,21)\n",
    "                # Q2 = np.random.randint(0,21)\n",
    "    \n",
    "    return Vn\n",
    "\n",
    "n = 3000\n",
    "\n",
    "Value_final_TD0 = TD0(1, n)\n",
    "print(Value_final_TD0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(Value_final_TD0,cmap='hot')\n",
    "plt.colorbar()\n",
    "plt.title('Value function for TD(0) with alpha = 1/sqrt(sqrt(n))')\n",
    "plt.xlabel('Q1')\n",
    "plt.ylabel('Q2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,3, figsize=(15, 4))\n",
    "ax_0=ax[0].imshow(Value_final,cmap='hot')\n",
    "ax[0].set_ylabel('Q2')\n",
    "ax[0].set_xlabel('Q1')\n",
    "ax[0].set_title('Random policy', fontsize=14)\n",
    "sm_0 = plt.cm.ScalarMappable(cmap=ax_0.get_cmap(), norm=ax_0.norm)\n",
    "sm_0.set_array([])\n",
    "plt.colorbar(sm_0, ax=ax[0], orientation='vertical')\n",
    "\n",
    "ax_1=ax[1].imshow(Value_final_TD0,cmap='hot')\n",
    "ax[1].set_ylabel('Q2')\n",
    "ax[1].set_xlabel('Q1')\n",
    "ax[1].set_title('TD(0)', fontsize=14)\n",
    "sm_1 = plt.cm.ScalarMappable(cmap=ax_1.get_cmap(), norm=ax_1.norm)\n",
    "sm_1.set_array([])\n",
    "plt.colorbar(sm_1, ax=ax[1], orientation='vertical')\n",
    "\n",
    "\n",
    "ax_2=ax[2].imshow(Value_final_TD0 - Value_final,cmap='hot')\n",
    "ax[2].set_ylabel('Q2')\n",
    "ax[2].set_xlabel('Q1')\n",
    "ax[2].set_title('Difference of them', fontsize=14)\n",
    "sm_2 = plt.cm.ScalarMappable(cmap=ax_2.get_cmap(), norm=ax_2.norm)\n",
    "sm_2.set_array([])\n",
    "plt.colorbar(sm_2, ax=ax[2], orientation='vertical')\n",
    "\n",
    "# Adjust the spacing between subplots\n",
    "plt.subplots_adjust(wspace=0.35)\n",
    "plt.show()\n",
    "print((Value_final_TD0 - Value_final) > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,3, figsize=(15, 4))\n",
    "ax_0=ax[0].imshow(Value_final_one_step_imp,cmap='hot')\n",
    "ax[0].set_ylabel('Q2')\n",
    "ax[0].set_xlabel('Q1')\n",
    "ax[0].set_title('With one step improvement', fontsize=14)\n",
    "sm_0 = plt.cm.ScalarMappable(cmap=ax_0.get_cmap(), norm=ax_0.norm)\n",
    "sm_0.set_array([])\n",
    "plt.colorbar(sm_0, ax=ax[0], orientation='vertical')\n",
    "\n",
    "ax_1=ax[1].imshow(Value_final_TD0,cmap='hot')\n",
    "ax[1].set_ylabel('Q2')\n",
    "ax[1].set_xlabel('Q1')\n",
    "ax[1].set_title('TD(0)', fontsize=14)\n",
    "sm_1 = plt.cm.ScalarMappable(cmap=ax_1.get_cmap(), norm=ax_1.norm)\n",
    "sm_1.set_array([])\n",
    "plt.colorbar(sm_1, ax=ax[1], orientation='vertical')\n",
    "\n",
    "\n",
    "ax_2=ax[2].imshow(Value_final_TD0 - Value_final_one_step_imp,cmap='hot')\n",
    "ax[2].set_ylabel('Q2')\n",
    "ax[2].set_xlabel('Q1')\n",
    "ax[2].set_title('Difference of them', fontsize=14)\n",
    "sm_2 = plt.cm.ScalarMappable(cmap=ax_2.get_cmap(), norm=ax_2.norm)\n",
    "sm_2.set_array([])\n",
    "plt.colorbar(sm_2, ax=ax[2], orientation='vertical')\n",
    "\n",
    "# Adjust the spacing between subplots\n",
    "plt.subplots_adjust(wspace=0.35)\n",
    "plt.show()\n",
    "print((Value_final_TD0 - Value_final_one_step_imp) > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,3, figsize=(15, 4))\n",
    "ax_0=ax[0].imshow(Value_final_optimal,cmap='hot')\n",
    "ax[0].set_ylabel('Q2')\n",
    "ax[0].set_xlabel('Q1')\n",
    "ax[0].set_title('Optimal value function', fontsize=14)\n",
    "sm_0 = plt.cm.ScalarMappable(cmap=ax_0.get_cmap(), norm=ax_0.norm)\n",
    "sm_0.set_array([])\n",
    "plt.colorbar(sm_0, ax=ax[0], orientation='vertical')\n",
    "\n",
    "ax_1=ax[1].imshow(Value_final_TD0,cmap='hot')\n",
    "ax[1].set_ylabel('Q2')\n",
    "ax[1].set_xlabel('Q1')\n",
    "ax[1].set_title('TD(0)', fontsize=14)\n",
    "sm_1 = plt.cm.ScalarMappable(cmap=ax_1.get_cmap(), norm=ax_1.norm)\n",
    "sm_1.set_array([])\n",
    "plt.colorbar(sm_1, ax=ax[1], orientation='vertical')\n",
    "\n",
    "\n",
    "ax_2=ax[2].imshow(Value_final_TD0 - Value_final_optimal,cmap='hot')\n",
    "ax[2].set_ylabel('Q2')\n",
    "ax[2].set_xlabel('Q1')\n",
    "ax[2].set_title('Difference of them', fontsize=14)\n",
    "sm_2 = plt.cm.ScalarMappable(cmap=ax_2.get_cmap(), norm=ax_2.norm)\n",
    "sm_2.set_array([])\n",
    "plt.colorbar(sm_2, ax=ax[2], orientation='vertical')\n",
    "\n",
    "# Adjust the spacing between subplots\n",
    "plt.subplots_adjust(wspace=0.35)\n",
    "plt.show()\n",
    "\n",
    "print((Value_final_TD0 - Value_final_optimal) > 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def racine_m(x,m):\n",
    "    for i in range(m):\n",
    "        x = math.sqrt(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement TD(0) with probability 0.5 to either queue 1 and 2, and take alpha = 1/n\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "lamda = 0.3\n",
    "\n",
    "mu1 = 0.2\n",
    "mu2 = 0.4\n",
    "\n",
    "gamma = 0.99\n",
    "\n",
    "p1 = 0.5\n",
    "\n",
    "p2 = 0.5\n",
    "# Simulate the environment\n",
    "\n",
    "def simulate_env(Q1, Q2, action):\n",
    "    Q1_minus_possible = True\n",
    "    Q2_minus_possible = True\n",
    "    Q1_plus_possible = True\n",
    "    Q2_plus_possible = True\n",
    "    one_action_done = False\n",
    "\n",
    "    if Q1 == 20:\n",
    "        Q1_plus_possible = False\n",
    "        if Q2 == 20:\n",
    "            Q2_plus_possible = False\n",
    "            action = 2\n",
    "        else:\n",
    "            Q2_plus_possible = True\n",
    "            action = 1\n",
    "    else:\n",
    "        Q1_plus_possible = True\n",
    "        if Q2 == 20:\n",
    "            Q2_plus_possible = False\n",
    "            action = 0\n",
    "        else:\n",
    "            Q2_plus_possible = True\n",
    "\n",
    "    if Q1 == 0:\n",
    "        Q1_minus_possible = False\n",
    "    else:\n",
    "        Q1_minus_possible = True\n",
    "    \n",
    "    if Q2 == 0:\n",
    "        Q2_minus_possible = False\n",
    "    else:\n",
    "        Q2_minus_possible = True\n",
    "\n",
    "    if action == 0:\n",
    "        if np.random.uniform() < lamda and Q1_plus_possible:\n",
    "            Q1 += 1\n",
    "            one_action_done = True\n",
    "    else:\n",
    "        if np.random.uniform() < lamda and Q2_plus_possible:\n",
    "            Q2 += 1\n",
    "            one_action_done = True\n",
    "    \n",
    "    if np.random.uniform() < mu1 and Q1_minus_possible and not one_action_done:\n",
    "        Q1 -= 1\n",
    "        one_action_done = True\n",
    "    if np.random.uniform() < mu2 and Q2_minus_possible and not one_action_done:\n",
    "        Q2 -= 1\n",
    "    \n",
    "    return Q1, Q2\n",
    "\n",
    "# Implement TD(0)\n",
    "\n",
    "def TD0(alpha, n):\n",
    "    Vn = np.zeros((21,21))\n",
    "    Tirage = np.zeros((21,21))\n",
    "    first_boucle = True\n",
    "    count = 0\n",
    "    Q1 = 0\n",
    "    Q2 = 0\n",
    "\n",
    "    while count < n:\n",
    "        count +=1\n",
    "        for Q1 in range(21):\n",
    "            for Q2 in range(21):\n",
    "                Tirage[Q1,Q2] +=1\n",
    "                # alpha = 1\n",
    "                alpha = 1/racine_m(Tirage[Q1,Q2],4)\n",
    "                # alpha = 1/math.sqrt(math.sqrt(Tirage[Q1,Q2]))\n",
    "                action = np.random.randint(0,2)\n",
    "                Q1_new, Q2_new = simulate_env(Q1, Q2, action)\n",
    "                Vn[Q1,Q2] = Vn[Q1,Q2] + alpha * (- (Q1 + Q2) + gamma * Vn[Q1_new,Q2_new] - Vn[Q1,Q2])\n",
    "\n",
    "                # Q1 = np.random.randint(0,21)\n",
    "                # Q2 = np.random.randint(0,21)\n",
    "    \n",
    "    return Vn\n",
    "\n",
    "n = 3000\n",
    "\n",
    "Value_final_TD0 = TD0(1, n)\n",
    "print(Value_final_TD0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(Value_final_TD0,cmap='hot')\n",
    "plt.colorbar()\n",
    "plt.title('Value function for TD(0) with alpha = 1/sqrt(sqrt(n))')\n",
    "plt.xlabel('Q1')\n",
    "plt.ylabel('Q2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,3, figsize=(15, 4))\n",
    "ax_0=ax[0].imshow(Value_final,cmap='hot')\n",
    "ax[0].set_ylabel('Q2')\n",
    "ax[0].set_xlabel('Q1')\n",
    "ax[0].set_title('Random policy', fontsize=14)\n",
    "sm_0 = plt.cm.ScalarMappable(cmap=ax_0.get_cmap(), norm=ax_0.norm)\n",
    "sm_0.set_array([])\n",
    "plt.colorbar(sm_0, ax=ax[0], orientation='vertical')\n",
    "\n",
    "ax_1=ax[1].imshow(Value_final_TD0,cmap='hot')\n",
    "ax[1].set_ylabel('Q2')\n",
    "ax[1].set_xlabel('Q1')\n",
    "ax[1].set_title('TD(0)', fontsize=14)\n",
    "sm_1 = plt.cm.ScalarMappable(cmap=ax_1.get_cmap(), norm=ax_1.norm)\n",
    "sm_1.set_array([])\n",
    "plt.colorbar(sm_1, ax=ax[1], orientation='vertical')\n",
    "\n",
    "\n",
    "ax_2=ax[2].imshow(Value_final_TD0 - Value_final,cmap='hot')\n",
    "ax[2].set_ylabel('Q2')\n",
    "ax[2].set_xlabel('Q1')\n",
    "ax[2].set_title('Difference of them', fontsize=14)\n",
    "sm_2 = plt.cm.ScalarMappable(cmap=ax_2.get_cmap(), norm=ax_2.norm)\n",
    "sm_2.set_array([])\n",
    "plt.colorbar(sm_2, ax=ax[2], orientation='vertical')\n",
    "\n",
    "# Adjust the spacing between subplots\n",
    "plt.subplots_adjust(wspace=0.35)\n",
    "plt.show()\n",
    "print((Value_final_TD0 - Value_final) > 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2 Optimal control\n",
    "\n",
    "In this part yo uare asked to find the optimal policy to dispatch incoming jobs\n",
    "- Implement Q-learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lamda = 0.3\n",
    "\n",
    "mu1 = 0.2\n",
    "mu2 = 0.4\n",
    "\n",
    "gamma = 0.99\n",
    "epsilon = 0.1\n",
    "\n",
    "\n",
    "def Q_learning(alpha, n):\n",
    "    Q_value = np.zeros((21,21,2))\n",
    "    Tirage = np.zeros((21,21))\n",
    "    count = 0\n",
    "    Q1 = 0\n",
    "    Q2 = 0\n",
    "\n",
    "    while count < n:\n",
    "        count +=1\n",
    "        Tirage[Q1,Q2] +=1\n",
    "        alpha = 1/Tirage[Q1,Q2]\n",
    "        #Deciding the optimal action using epsilon-greedy\n",
    "        if np.random.rand() < epsilon:\n",
    "            action = np.random.randint(0,2)\n",
    "        else:\n",
    "            action = np.argmax(Q_value[Q1,Q2])\n",
    "        \n",
    "        #Simulating a new state\n",
    "        Q1_new, Q2_new = simulate_env(Q1, Q2, action)\n",
    "        #Measuring reward\n",
    "        reward = - (Q1 + Q2)\n",
    "        Q_value[Q1,Q2,action] = Q_value[Q1,Q2, action] + alpha * ( reward + gamma * np.max(Q_value[Q1_new,Q2_new]) - Q_value[Q1,Q2, action])\n",
    "\n",
    "        Q1 = Q1_new\n",
    "        Q2 = Q2_new\n",
    "\n",
    "    return Q_value\n",
    "\n",
    "n = 1000000\n",
    "\n",
    "Final_Q_Value = Q_learning(1, n)\n",
    "print(Final_Q_Value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def action_optimal(Q_value):\n",
    "    n = len(Q_value)\n",
    "    action_matrix = np.zeros((n,n))\n",
    "    max_q_value_matrix = np.zeros((n,n))\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            action_matrix[i,j] = np.argmax(Q_value[i,j])\n",
    "            max_q_value_matrix[i,j] = np.max(Q_value[i,j])\n",
    "\n",
    "    return action_matrix, max_q_value_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_matrix, max_Q_value = action_optimal(Q_value = Final_Q_Value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2)\n",
    "ax_0=ax[0].imshow(Final_Q_Value[:,:,0],cmap='hot')\n",
    "ax[0].set_ylabel('Q2')\n",
    "ax[0].set_xlabel('Q1')\n",
    "ax[0].set_title('Q value for action 0', fontsize=14)\n",
    "sm_0 = plt.cm.ScalarMappable(cmap=ax_0.get_cmap(), norm=ax_0.norm)\n",
    "sm_0.set_array([])\n",
    "plt.colorbar(sm_0, ax=ax[0], orientation='vertical')\n",
    "\n",
    "ax_1=ax[1].imshow(Final_Q_Value[:,:,1],cmap='hot')\n",
    "ax[1].set_ylabel('Q2')\n",
    "ax[1].set_xlabel('Q1')\n",
    "ax[1].set_title('Q value for action 1', fontsize=14)\n",
    "sm_1 = plt.cm.ScalarMappable(cmap=ax_1.get_cmap(), norm=ax_1.norm)\n",
    "sm_1.set_array([])\n",
    "plt.colorbar(sm_1, ax=ax[1], orientation='vertical')\n",
    "\n",
    "# Adjust the spacing between subplots\n",
    "plt.subplots_adjust(wspace=0.3)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(max_Q_value,cmap = 'hot')\n",
    "plt.colorbar()\n",
    "plt.title('Q value for optimal action')\n",
    "plt.xlabel('Q1')\n",
    "plt.ylabel('Q2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,3, figsize=(15, 4))\n",
    "ax_0=ax[0].imshow(Final_Q_Value[:,:,0],cmap='hot')\n",
    "ax[0].set_ylabel('Q2')\n",
    "ax[0].set_xlabel('Q1')\n",
    "ax[0].set_title('Q value for action 0', fontsize=14)\n",
    "sm_0 = plt.cm.ScalarMappable(cmap=ax_0.get_cmap(), norm=ax_0.norm)\n",
    "sm_0.set_array([])\n",
    "plt.colorbar(sm_0, ax=ax[0], orientation='vertical')\n",
    "\n",
    "ax_1=ax[1].imshow(Final_Q_Value[:,:,1],cmap='hot')\n",
    "ax[1].set_ylabel('Q2')\n",
    "ax[1].set_xlabel('Q1')\n",
    "ax[1].set_title('Q value for action 1', fontsize=14)\n",
    "sm_1 = plt.cm.ScalarMappable(cmap=ax_1.get_cmap(), norm=ax_1.norm)\n",
    "sm_1.set_array([])\n",
    "plt.colorbar(sm_1, ax=ax[1], orientation='vertical')\n",
    "\n",
    "ax_2=ax[2].imshow(max_Q_value,cmap='hot')\n",
    "ax[2].set_ylabel('Q2')\n",
    "ax[2].set_xlabel('Q1')\n",
    "ax[2].set_title('Q value for optimal action', fontsize=14)\n",
    "sm_2 = plt.cm.ScalarMappable(cmap=ax_2.get_cmap(), norm=ax_2.norm)\n",
    "sm_2.set_array([])\n",
    "plt.colorbar(sm_2, ax=ax[2], orientation='vertical')\n",
    "\n",
    "# Adjust the spacing between subplots\n",
    "plt.subplots_adjust(wspace=0.3)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Represent on the plane the optimal action as a function of the state ($Q_1$,$Q_2$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vecteurs = np.zeros((21,21,2))\n",
    "for i in range(21):\n",
    "    for j in range(21):\n",
    "        if action_matrix[i,j] == 0:\n",
    "            vecteurs[i,j] = [1,0]\n",
    "        else:\n",
    "            vecteurs[i,j] = [0,1]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(max_Q_value,cmap='hot')\n",
    "ax.quiver(vecteurs[:,:,1], vecteurs[:,:,0], angles='xy', scale_units = 'xy', scale=1)\n",
    "plt.title('Optimal action')\n",
    "plt.xlabel('Q1')\n",
    "plt.ylabel('Q2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Explore other alternatives for $\\alpha_n$, for example constant or $\\frac{1}{n}$?, to see if convergence improves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lamda = 0.3\n",
    "\n",
    "mu1 = 0.2\n",
    "mu2 = 0.4\n",
    "\n",
    "gamma = 0.99\n",
    "epsilon = 0.1\n",
    "\n",
    "\n",
    "def Q_learning(alpha, n):\n",
    "    Q_value = np.zeros((21,21,2))\n",
    "    Tirage = np.zeros((21,21))\n",
    "    count = 0\n",
    "    Q1 = 0\n",
    "    Q2 = 0\n",
    "\n",
    "    while count < n:\n",
    "        count +=1\n",
    "        Tirage[Q1,Q2] +=1\n",
    "        # alpha = 1/Tirage[Q1,Q2]\n",
    "        alpha = 1/math.pow(Tirage[Q1,Q2],1.5)\n",
    "        #Deciding the optimal action using epsilon-greedy\n",
    "        if np.random.rand() < epsilon:\n",
    "            action = np.random.randint(0,2)\n",
    "        else:\n",
    "            action = np.argmax(Q_value[Q1,Q2])\n",
    "        \n",
    "        #Simulating a new state\n",
    "        Q1_new, Q2_new = simulate_env(Q1, Q2, action)\n",
    "        #Measuring reward\n",
    "        reward = - (Q1 + Q2)\n",
    "        Q_value[Q1,Q2,action] = Q_value[Q1,Q2, action] + alpha * ( reward + gamma * np.max(Q_value[Q1_new,Q2_new]) - Q_value[Q1,Q2, action])\n",
    "\n",
    "        Q1 = Q1_new\n",
    "        Q2 = Q2_new\n",
    "\n",
    "    return Q_value\n",
    "\n",
    "n = 1000000\n",
    "\n",
    "Final_Q_Value = Q_learning(1, n)\n",
    "action_matrix, max_Q_value = action_optimal(Q_value = Final_Q_Value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,3, figsize=(15, 4))\n",
    "ax_0=ax[0].imshow(Final_Q_Value[:,:,0],cmap='hot')\n",
    "ax[0].set_ylabel('Q2')\n",
    "ax[0].set_xlabel('Q1')\n",
    "ax[0].set_title('Q value for action 0', fontsize=14)\n",
    "sm_0 = plt.cm.ScalarMappable(cmap=ax_0.get_cmap(), norm=ax_0.norm)\n",
    "sm_0.set_array([])\n",
    "plt.colorbar(sm_0, ax=ax[0], orientation='vertical')\n",
    "\n",
    "ax_1=ax[1].imshow(Final_Q_Value[:,:,1],cmap='hot')\n",
    "ax[1].set_ylabel('Q2')\n",
    "ax[1].set_xlabel('Q1')\n",
    "ax[1].set_title('Q value for action 1', fontsize=14)\n",
    "sm_1 = plt.cm.ScalarMappable(cmap=ax_1.get_cmap(), norm=ax_1.norm)\n",
    "sm_1.set_array([])\n",
    "plt.colorbar(sm_1, ax=ax[1], orientation='vertical')\n",
    "\n",
    "ax_2=ax[2].imshow(max_Q_value,cmap='hot')\n",
    "ax[2].set_ylabel('Q2')\n",
    "ax[2].set_xlabel('Q1')\n",
    "ax[2].set_title('Q value for optimal action', fontsize=14)\n",
    "sm_2 = plt.cm.ScalarMappable(cmap=ax_2.get_cmap(), norm=ax_2.norm)\n",
    "sm_2.set_array([])\n",
    "plt.colorbar(sm_2, ax=ax[2], orientation='vertical')\n",
    "\n",
    "# Adjust the spacing between subplots\n",
    "plt.subplots_adjust(wspace=0.3)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vecteurs = np.zeros((21,21,2))\n",
    "for i in range(21):\n",
    "    for j in range(21):\n",
    "        if action_matrix[i,j] == 0:\n",
    "            vecteurs[i,j] = [1,0]\n",
    "        else:\n",
    "            vecteurs[i,j] = [0,1]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(max_Q_value,cmap='hot')\n",
    "ax.quiver(vecteurs[:,:,1], vecteurs[:,:,0], angles='xy', scale_units = 'xy', scale=1)\n",
    "plt.title('Optimal action')\n",
    "plt.xlabel('Q1')\n",
    "plt.ylabel('Q2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lamda = 0.3\n",
    "\n",
    "mu1 = 0.2\n",
    "mu2 = 0.4\n",
    "\n",
    "gamma = 0.99\n",
    "epsilon = 0.1\n",
    "\n",
    "\n",
    "def Q_learning(alpha, n):\n",
    "    Q_value = np.zeros((21,21,2))\n",
    "    Tirage = np.zeros((21,21))\n",
    "    count = 0\n",
    "    Q1 = 0\n",
    "    Q2 = 0\n",
    "\n",
    "    while count < n:\n",
    "        count +=1\n",
    "        Tirage[Q1,Q2] +=1\n",
    "        alpha = 1\n",
    "        # alpha = 1/math.sqrt(math.sqrt(Tirage[Q1,Q2]))\n",
    "        #Deciding the optimal action using epsilon-greedy\n",
    "        if np.random.rand() < epsilon:\n",
    "            action = np.random.randint(0,2)\n",
    "        else:\n",
    "            action = np.argmax(Q_value[Q1,Q2])\n",
    "        \n",
    "        #Simulating a new state\n",
    "        Q1_new, Q2_new = simulate_env(Q1, Q2, action)\n",
    "        #Measuring reward\n",
    "        reward = - (Q1 + Q2)\n",
    "        Q_value[Q1,Q2,action] = Q_value[Q1,Q2, action] + alpha * ( reward + gamma * np.max(Q_value[Q1_new,Q2_new]) - Q_value[Q1,Q2, action])\n",
    "\n",
    "        Q1 = Q1_new\n",
    "        Q2 = Q2_new\n",
    "\n",
    "    return Q_value\n",
    "\n",
    "n = 1000000\n",
    "\n",
    "Final_Q_Value = Q_learning(1, n)\n",
    "action_matrix, max_Q_value = action_optimal(Q_value = Final_Q_Value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,3, figsize=(15, 4))\n",
    "ax_0=ax[0].imshow(Final_Q_Value[:,:,0],cmap='hot')\n",
    "ax[0].set_ylabel('Q2')\n",
    "ax[0].set_xlabel('Q1')\n",
    "ax[0].set_title('Q value for action 0', fontsize=14)\n",
    "sm_0 = plt.cm.ScalarMappable(cmap=ax_0.get_cmap(), norm=ax_0.norm)\n",
    "sm_0.set_array([])\n",
    "plt.colorbar(sm_0, ax=ax[0], orientation='vertical')\n",
    "\n",
    "ax_1=ax[1].imshow(Final_Q_Value[:,:,1],cmap='hot')\n",
    "ax[1].set_ylabel('Q2')\n",
    "ax[1].set_xlabel('Q1')\n",
    "ax[1].set_title('Q value for action 1', fontsize=14)\n",
    "sm_1 = plt.cm.ScalarMappable(cmap=ax_1.get_cmap(), norm=ax_1.norm)\n",
    "sm_1.set_array([])\n",
    "plt.colorbar(sm_1, ax=ax[1], orientation='vertical')\n",
    "\n",
    "ax_2=ax[2].imshow(max_Q_value,cmap='hot')\n",
    "ax[2].set_ylabel('Q2')\n",
    "ax[2].set_xlabel('Q1')\n",
    "ax[2].set_title('Q value for optimal action', fontsize=14)\n",
    "sm_2 = plt.cm.ScalarMappable(cmap=ax_2.get_cmap(), norm=ax_2.norm)\n",
    "sm_2.set_array([])\n",
    "plt.colorbar(sm_2, ax=ax[2], orientation='vertical')\n",
    "\n",
    "# Adjust the spacing between subplots\n",
    "plt.subplots_adjust(wspace=0.3)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vecteurs = np.zeros((21,21,2))\n",
    "for i in range(21):\n",
    "    for j in range(21):\n",
    "        if action_matrix[i,j] == 0:\n",
    "            vecteurs[i,j] = [1,0]\n",
    "        else:\n",
    "            vecteurs[i,j] = [0,1]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(max_Q_value,cmap='hot')\n",
    "ax.quiver(vecteurs[:,:,1], vecteurs[:,:,0], angles='xy', scale_units = 'xy', scale=1)\n",
    "plt.title('Optimal action')\n",
    "plt.xlabel('Q1')\n",
    "plt.ylabel('Q2')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
